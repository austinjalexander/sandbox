{
 "metadata": {
  "name": "",
  "signature": "sha256:ea691eba0522287ad2b3dffdd64fd41002608ade95fe3f40987158036b3fc724"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Supervised Learning Problem\n",
      "\n",
      "outcome measurement $Y$ (dependent variable, response, target)\n",
      "\n",
      "vector of $p$ predictor measurements $X$ (inputs, regressors, covariates, features, independent variables)\n",
      "\n",
      "regression problem: $Y$ is quantitative \n",
      "\n",
      "classification problem: $Y$ takes values in a finite, unordered set\n",
      "\n",
      "training data: $(x_{1}, y_{1}),...,(x_{N}, y_{N})$ (observations [examples, instances] of these measurements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "objectives: accurately predict unseen test cases, understand which inputs affect the outcome and how, assess the quality of our predictions and inferences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##unsupervised learning\n",
      "\n",
      "no outcome variable, just a set of predictors (features) measured on a set of samples\n",
      "\n",
      "objective is more fuzzy -- find groups of samples that behave similarly, find features that behave similarly, find linear combinations of features with the most variation\n",
      "\n",
      "difficult to know how well you are doing\n",
      "\n",
      "different from supervised learning, but can be useful as a pre-processing step for supervised learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "input vector  \n",
      "$X = \\begin{pmatrix}X_{1}\\\\X_{2}\\\\X_{3}\\end{pmatrix}$\n",
      "\n",
      "model  \n",
      "$Y = f(X) + \\epsilon$ (where $\\epsilon$ captures errors and other discrepancies\n",
      "\n",
      "with a good $f$, can make predicitons of $Y$ at new points $X = x$\n",
      "\n",
      "can understand which components of $X = (X_{1},X_{2},...,X_{p})$ are important in explaining $Y$ and which are irrelevant\n",
      "\n",
      "depending on complexity of $F$, may be able to understand how each component $X_{j}$ of $X$ affects $Y$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ideal $f(X)$?\n",
      "\n",
      "a good value is, for example, for $4$, $f(4) = E(Y|X=4)$\n",
      "\n",
      "$E(Y|X=4)$ means expected value (conditional average) of $Y$ given $X=4$\n",
      "\n",
      "this ideal $f(x) = E(Y|X=x)$ is called the regression function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "regression function $f(x)$ is also defined for vector $X$\n",
      "\n",
      "$f(x) = f(x_{1},x_{2},x_{3}) = E(Y|X_{1} = x_{1}, X_{2} = x_{2}, X_{3} = x_{3})$\n",
      "\n",
      "it's the ideal or optimal (with regards to a loss function, the mean-squared prediction error) predictor of $Y$\n",
      "\n",
      "$f(x) = E(Y|X=x)$ is the function that minimizes $E[(Y-g(X))^{2}|X=x]$ over all functions $g$ at all points $X=x$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\epsilon = Y - f(x)$ is the irreducible error; even if $f(x)$ is known, still errors in prediction, since at each $X=x$ there is typically a distribution of possible $Y$ values\n",
      "\n",
      "for any estimate $\\hat{f}(x)$ of $f(x)$:  \n",
      "$E[(Y-g(X))^{2}|X=x] = [f(x) - \\hat{f}(x)]^{2} + \\text{Var}(\\epsilon)$  \n",
      "$[f(x) - \\hat{f}(x)]^{2}$ is reducible\n",
      "$\\text{Var}(\\epsilon)$ is irreducible"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "typically few if any data points with $X=4$, so cannot compute $E(Y|X=x))\n",
      "\n",
      "thus, relax definition and let $\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))$, where $\\mathcal{N}(x)$ is some neighborhood of $x$\n",
      "\n",
      "this is called nearest-neighbor or local averaging"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}