{
 "metadata": {
  "name": "",
  "signature": "sha256:ab4ea8b981bba99ea13873c397ec02d22fd6d2a1026ab63ef14878e4aad86308"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "dataframe = pd.read_csv('turnstile_data_master_with_weather.csv', nrows=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy\n",
      "import pandas\n",
      "from ggplot import *\n",
      "\n",
      "\"\"\"\n",
      "In this question, you need to:\n",
      "1) implement the compute_cost() and gradient_descent() procedures\n",
      "2) Select features (in the predictions procedure) and make predictions.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "def normalize_features(array):\n",
      "   \"\"\"\n",
      "   Normalize the features in the data set.\n",
      "   \"\"\"\n",
      "   array_normalized = (array-array.mean())/array.std()\n",
      "   mu = array.mean()\n",
      "   sigma = array.std()\n",
      "\n",
      "   return array_normalized, mu, sigma\n",
      "\n",
      "def compute_cost(features, values, theta):\n",
      "    \"\"\"\n",
      "    Compute the cost function given a set of features / values, \n",
      "    and the values for our thetas.\n",
      "    \n",
      "    This can be the same code as the compute_cost function in the lesson #3 exercises,\n",
      "    but feel free to implement your own.\n",
      "    \"\"\"\n",
      "    \n",
      "    m = len(values)\n",
      "    sum_of_square_errors = np.square(np.dot(features, theta) - values).sum()\n",
      "    cost = sum_of_square_errors / (2.0*m)\n",
      "\n",
      "    return cost\n",
      "\n",
      "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
      "    \"\"\"\n",
      "    Perform gradient descent given a data set with an arbitrary number of features.\n",
      "    \n",
      "    This can be the same gradient descent code as in the lesson #3 exercises,\n",
      "    but feel free to implement your own.\n",
      "    \"\"\"\n",
      "    \n",
      "    m = len(values)\n",
      "    \n",
      "    cost_history = []\n",
      "    \n",
      "    for iteration in xrange(0,num_iterations):\n",
      "\n",
      "        new_theta = theta\n",
      "        for j in xrange(0,len(theta)):\n",
      "            #print j, features[:,j]\n",
      "            new_theta[j] = theta[j] + ((1.0*alpha)/m) * ((values - np.dot(features, theta)).dot(features[:,j]))\n",
      "        \n",
      "        theta = new_theta\n",
      "        \n",
      "        cost_history.append(compute_cost(features, values, theta))\n",
      "        \n",
      "    return theta, pandas.Series(cost_history)\n",
      "\n",
      "def predictions(dataframe):\n",
      "    '''\n",
      "    The NYC turnstile data is stored in a pandas dataframe called weather_turnstile.\n",
      "    Using the information stored in the dataframe, let's predict the ridership of\n",
      "    the NYC subway using linear regression with gradient descent.\n",
      "    \n",
      "    You can download the complete turnstile weather dataframe here:\n",
      "    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv    \n",
      "    \n",
      "    Your prediction should have a R^2 value of 0.20 or better.\n",
      "    You need to experiment using various input features contained in the dataframe. \n",
      "    We recommend that you don't use the EXITSn_hourly feature as an input to the \n",
      "    linear model because we cannot use it as a predictor: we cannot use exits \n",
      "    counts as a way to predict entry counts. \n",
      "    \n",
      "    Note: Due to the memory and CPU limitation of our Amazon EC2 instance, we will\n",
      "    give you a random subet (~15%) of the data contained in \n",
      "    turnstile_data_master_with_weather.csv. You are encouraged to experiment with \n",
      "    this computer on your own computer, locally. \n",
      "    \n",
      "    \n",
      "    If you'd like to view a plot of your cost history, uncomment the call to \n",
      "    plot_cost_history below. The slowdown from plotting is significant, so if you \n",
      "    are timing out, the first thing to do is to comment out the plot command again.\n",
      "    \n",
      "    If you receive a \"server has encountered an error\" message, that means you are \n",
      "    hitting the 30-second limit that's placed on running your program. Try using a \n",
      "    smaller number for num_iterations if that's the case.\n",
      "    \n",
      "    If you are using your own algorithm/models, see if you can optimize your code so \n",
      "    that it runs faster.\n",
      "    '''\n",
      "\n",
      "    # Select Features (try different features!)\n",
      "    #features = dataframe[['Hour', 'maxpressurei', 'maxdewpti', 'mindewpti', 'meandewpti', 'meanpressurei', 'mintempi', 'maxtempi']].copy()\n",
      "    features = dataframe[['Hour', 'maxpressurei', 'maxtempi']].copy()\n",
      "    \n",
      "    # Add UNIT to features using dummy variables\n",
      "    dummy_units = pandas.get_dummies(dataframe['UNIT'], prefix='unit')\n",
      "    features = features.join(dummy_units)\n",
      "    \n",
      "    # Values\n",
      "    values = dataframe[['ENTRIESn_hourly']]\n",
      "    m = len(values)\n",
      "\n",
      "    features, mu, sigma = normalize_features(features)\n",
      "    features['ones'] = np.ones(m) # Add a column of 1s (y intercept)\n",
      "    print features.head()\n",
      "    \n",
      "    # Convert features and values to numpy arrays\n",
      "    features_array = np.array(features)\n",
      "    values_array = np.array(values).flatten()\n",
      "\n",
      "    # Set values for alpha, number of iterations.\n",
      "    alpha = 0.1 # please feel free to change this value\n",
      "    num_iterations = 100 # please feel free to change this value\n",
      "\n",
      "    # Initialize theta, perform gradient descent\n",
      "    theta_gradient_descent = np.zeros(len(features.columns))\n",
      "    theta_gradient_descent, cost_history = gradient_descent(features_array, \n",
      "                                                            values_array, \n",
      "                                                            theta_gradient_descent, \n",
      "                                                            alpha, \n",
      "                                                            num_iterations)\n",
      "    \n",
      "    print theta_gradient_descent\n",
      "    \n",
      "    plot = None\n",
      "    # -------------------------------------------------\n",
      "    # Uncomment the next line to see your cost history\n",
      "    # -------------------------------------------------\n",
      "    plot = plot_cost_history(alpha, cost_history)\n",
      "    # \n",
      "    # Please note, there is a possibility that plotting\n",
      "    # this in addition to your calculation will exceed \n",
      "    # the 30 second limit on the compute servers.\n",
      "    \n",
      "    \n",
      "    predictions = np.dot(features_array, theta_gradient_descent)\n",
      "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(features, values)\n",
      "    print str(1 - np.sum((np.square(values.values - predictions)))/np.sum(np.square(values.values - np.mean(values.values))))\n",
      "    print r_value**2\n",
      "    return predictions, plot\n",
      "\n",
      "\n",
      "def plot_cost_history(alpha, cost_history):\n",
      "   \"\"\"This function is for viewing the plot of your cost history.\n",
      "   You can run it by uncommenting this\n",
      "\n",
      "       plot_cost_history(alpha, cost_history) \n",
      "\n",
      "   call in predictions.\n",
      "   \n",
      "   If you want to run this locally, you should print the return value\n",
      "   from this function.\n",
      "   \"\"\"\n",
      "   cost_df = pandas.DataFrame({\n",
      "      'Cost_History': cost_history,\n",
      "      'Iteration': range(len(cost_history))\n",
      "   })\n",
      "   return ggplot(cost_df, aes('Iteration', 'Cost_History')) + \\\n",
      "      geom_point() + ggtitle('Cost History for alpha = %.3f' % alpha )\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions(dataframe)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       Hour  maxpressurei  maxtempi  unit_R001  unit_R002  unit_R003  \\\n",
        "0 -1.306898  2.269830e-08       NaN   3.938274  -0.251379  -0.228266   \n",
        "1 -0.738681  2.269830e-08       NaN   3.938274  -0.251379  -0.228266   \n",
        "2 -0.170465  2.269830e-08       NaN   3.938274  -0.251379  -0.228266   \n",
        "3  0.397752  2.269830e-08       NaN   3.938274  -0.251379  -0.228266   \n",
        "4  0.965968  2.269830e-08       NaN   3.938274  -0.251379  -0.228266   \n",
        "\n",
        "   unit_R004  unit_R005  unit_R006  unit_R007    ...      unit_R010  \\\n",
        "0  -0.251379  -0.228266  -0.251379  -0.203101    ...      -0.251379   \n",
        "1  -0.251379  -0.228266  -0.251379  -0.203101    ...      -0.251379   \n",
        "2  -0.251379  -0.228266  -0.251379  -0.203101    ...      -0.251379   \n",
        "3  -0.251379  -0.228266  -0.251379  -0.203101    ...      -0.251379   \n",
        "4  -0.251379  -0.228266  -0.251379  -0.203101    ...      -0.251379   \n",
        "\n",
        "   unit_R011  unit_R012  unit_R013  unit_R014  unit_R015  unit_R016  \\\n",
        "0  -0.251379  -0.272976  -0.251379  -0.251379  -0.251379  -0.228266   \n",
        "1  -0.251379  -0.272976  -0.251379  -0.251379  -0.251379  -0.228266   \n",
        "2  -0.251379  -0.272976  -0.251379  -0.251379  -0.251379  -0.228266   \n",
        "3  -0.251379  -0.272976  -0.251379  -0.251379  -0.251379  -0.228266   \n",
        "4  -0.251379  -0.272976  -0.251379  -0.251379  -0.251379  -0.228266   \n",
        "\n",
        "   unit_R017  unit_R018  ones  \n",
        "0  -0.251379  -0.203101     1  \n",
        "1  -0.251379  -0.203101     1  \n",
        "2  -0.251379  -0.203101     1  \n",
        "3  -0.251379  -0.203101     1  \n",
        "4  -0.251379  -0.203101     1  \n",
        "\n",
        "[5 rows x 22 columns]\n",
        "[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
        "  nan  nan  nan  nan  nan  nan  nan]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-770d36682877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-2-22464cd5528b>\u001b[0m in \u001b[0;36mpredictions\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_gradient_descent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mslope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinregress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mr_value\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/scipy/stats/stats.pyc\u001b[0m in \u001b[0;36mlinregress\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[0;31m# average sum of squares:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3011\u001b[0;31m     \u001b[0mssxm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssxym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssyxm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3012\u001b[0m     \u001b[0mr_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssxym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m     \u001b[0mr_den\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssxm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mssym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof)\u001b[0m\n\u001b[1;32m   1881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}