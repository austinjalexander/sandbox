{
 "metadata": {
  "name": "",
  "signature": "sha256:9b6b44b61e91a50cb40e68a4962807e257afe9af324f6ff00e6762bdc6c31dea"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# Your task here is to extract data from xml on authors of an article\n",
      "# and add it to a list, one item for an author.\n",
      "# See the provided data structure for the expected format.\n",
      "# The tags for first name, surname and email should map directly\n",
      "# to the dictionary keys\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "article_file = \"exampleResearchArticle.xml\"\n",
      "\n",
      "\n",
      "def get_root(fname):\n",
      "    tree = ET.parse(fname)\n",
      "    return tree.getroot()\n",
      "\n",
      "\n",
      "def get_authors(root):\n",
      "    authors = []\n",
      "    for author in root.findall('./fm/bibl/aug/au'):\n",
      "        data = {\n",
      "                \"fnm\": author.find('fnm').text,\n",
      "                \"snm\": author.find('snm').text,\n",
      "                \"email\": author.find('email').text\n",
      "        }\n",
      "\n",
      "        authors.append(data)\n",
      "\n",
      "    return authors\n",
      "\n",
      "\n",
      "def test():\n",
      "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
      "    \n",
      "    root = get_root(article_file)\n",
      "    data = get_authors(root)\n",
      "\n",
      "    assert data[0] == solution[0]\n",
      "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
      "\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One way of solving the exercise is listed below. There are couple of things to note about this solution:\n",
      "\n",
      "since the names and email for authors are unique for each author, we use \"find\".\n",
      "The text method can be used to get the value of the tag.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_author(root):\n",
      "    authors = []\n",
      "    for author in root.findall('./fm/bibl/aug/au'):\n",
      "        data = {\n",
      "                \"fnm\": None,\n",
      "                \"snm\": None,\n",
      "                \"email\": None\n",
      "        }\n",
      "        data[\"fnm\"] = author.find('./fnm').text\n",
      "        data[\"snm\"] = author.find('./snm').text\n",
      "        data[\"email\"] = author.find('./email').text\n",
      "\n",
      "        authors.append(data)\n",
      "\n",
      "    return authors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "tree = ET.parse(\"exampleResearchArticle.xml\")\n",
      "root = tree.getroot()\n",
      "authors = []\n",
      "for author in root.findall('./fm/bibl/aug/au'):\n",
      "    print [insr.attrib['iid'] for insr in author.findall('insr')]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['I1']\n",
        "['I2']\n",
        "['I3', 'I4']\n",
        "['I3']\n",
        "['I8']\n",
        "['I3', 'I5']\n",
        "['I6']\n",
        "['I7']\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# Your task here is to extract data from xml on authors of an article\n",
      "# and add it to a list, one item for an author.\n",
      "# See the provided data structure for the expected format.\n",
      "# The tags for first name, surname and email should map directly\n",
      "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
      "# and add them to the list for the dictionary key \"insr\"\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "article_file = \"exampleResearchArticle.xml\"\n",
      "\n",
      "\n",
      "def get_root(fname):\n",
      "    tree = ET.parse(fname)\n",
      "    return tree.getroot()\n",
      "\n",
      "\n",
      "def get_authors(root):\n",
      "    authors = []\n",
      "    for author in root.findall('./fm/bibl/aug/au'):\n",
      "        data = {\n",
      "                \"fnm\": author.find('fnm').text,\n",
      "                \"snm\": author.find('snm').text,\n",
      "                \"email\": author.find('email').text,\n",
      "                \"insr\": [insr.attrib['iid'] for insr in author.findall('insr')]\n",
      "        }\n",
      "\n",
      "        authors.append(data)\n",
      "\n",
      "    return authors\n",
      "\n",
      "\n",
      "def test():\n",
      "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
      "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
      "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
      "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
      "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
      "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
      "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
      "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
      "\n",
      "    root = get_root(article_file)\n",
      "    data = get_authors(root)\n",
      "\n",
      "    assert data[0] == solution[0]\n",
      "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
      "\n",
      "\n",
      "test()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One way of solving the exercise is listed below. There are couple of things to note about this solution:\n",
      "\n",
      "since the names and email for authors are unique for each author, we use \"find\". The text method can be used to get the value of the tag.\n",
      "since \"insr\" can contain several values, we use \"findall\" and iterate the returned list.\n",
      "we access the attributes of a tag by method \"attrib\" and using the attribute name \"iid\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_authors(root):\n",
      "    authors = []\n",
      "    for author in root.findall('./fm/bibl/aug/au'):\n",
      "        data = {\n",
      "                \"fnm\": None,\n",
      "                \"snm\": None,\n",
      "                \"email\": None,\n",
      "                \"insr\": []\n",
      "        }\n",
      "        data[\"fnm\"] = author.find('./fnm').text\n",
      "        data[\"snm\"] = author.find('./snm').text\n",
      "        data[\"email\"] = author.find('./email').text\n",
      "        insr = author.findall('./insr')\n",
      "        for i in insr:\n",
      "            data[\"insr\"].append(i.attrib[\"iid\"])\n",
      "        authors.append(data)\n",
      "\n",
      "    return authors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# Please note that the function 'make_request' is provided for your reference only.\n",
      "# You will not be able to to actually use it from within the Udacity web UI.\n",
      "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
      "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the approprate\n",
      "# values in the data dictionary.\n",
      "# All your changes should be in the 'extract_data' function\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import json\n",
      "\n",
      "html_page = \"page_source.html\"\n",
      "\n",
      "\n",
      "def extract_data(page):\n",
      "    data = {\"eventvalidation\": \"\",\n",
      "            \"viewstate\": \"\"}\n",
      "    with open(page, \"r\") as html:\n",
      "        page = BeautifulSoup(html)\n",
      "        \n",
      "        data['eventvalidation'] = page.find_all(id='__EVENTVALIDATION')[0]['value']\n",
      "        data['viewstate'] = page.find_all(id='__VIEWSTATE')[0]['value']\n",
      "        \n",
      "    return data\n",
      "\n",
      "\n",
      "def make_request(data):\n",
      "    eventvalidation = data[\"eventvalidation\"]\n",
      "    viewstate = data[\"viewstate\"]\n",
      "\n",
      "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
      "                    data={'AirportList': \"BOS\",\n",
      "                          'CarrierList': \"VX\",\n",
      "                          'Submit': 'Submit',\n",
      "                          \"__EVENTTARGET\": \"\",\n",
      "                          \"__EVENTARGUMENT\": \"\",\n",
      "                          \"__EVENTVALIDATION\": eventvalidation,\n",
      "                          \"__VIEWSTATE\": viewstate\n",
      "                    })\n",
      "\n",
      "    return r.text\n",
      "\n",
      "\n",
      "def test():\n",
      "    data = extract_data(html_page)\n",
      "    assert data[\"eventvalidation\"] != \"\"\n",
      "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
      "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
      "\n",
      "    \n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_data(page):\n",
      "    data = {\"eventvalidation\": \"\",\n",
      "            \"viewstate\": \"\"}\n",
      "    with open(page, \"r\") as html:\n",
      "        soup = BeautifulSoup(html)\n",
      "        ev = soup.find(id=\"__EVENTVALIDATION\")\n",
      "        data[\"eventvalidation\"] = ev[\"value\"]\n",
      "\n",
      "        vs = soup.find(id=\"__VIEWSTATE\")\n",
      "        data[\"viewstate\"] = vs[\"value\"]\n",
      "\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# Please note that the function 'make_request' is provided for your reference only.\n",
      "# You will not be able to to actually use it from within the Udacity web UI\n",
      "# All your changes should be in the 'extract_carrier' function\n",
      "# Also note that the html file is a stripped down version of what is actually on the website.\n",
      "\n",
      "# Your task in this exercise is to get a list of all airlines. Exclude all of the combination\n",
      "# values, like \"All U.S. Carriers\" from the data that you return.\n",
      "# You should return a list of codes for the carriers.\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "html_page = \"options.html\"\n",
      "\n",
      "\n",
      "def extract_carriers(page):\n",
      "    data = []\n",
      "\n",
      "    with open(page, \"r\") as html:\n",
      "        # do something here to find the necessary values\n",
      "        soup = BeautifulSoup(html)\n",
      "        \n",
      "        carrier_list = soup.find_all(id='CarrierList')[0]\n",
      "        options = carrier_list.find_all('option')\n",
      "        \n",
      "        for option in options:\n",
      "            if 'All' not in option['value']:\n",
      "                data.append(option['value'])\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def make_request(data):\n",
      "    eventvalidation = data[\"eventvalidation\"]\n",
      "    viewstate = data[\"viewstate\"]\n",
      "    airport = data[\"airport\"]\n",
      "    carrier = data[\"carrier\"]\n",
      "\n",
      "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
      "                    data={'AirportList': airport,\n",
      "                          'CarrierList': carrier,\n",
      "                          'Submit': 'Submit',\n",
      "                          \"__EVENTTARGET\": \"\",\n",
      "                          \"__EVENTARGUMENT\": \"\",\n",
      "                          \"__EVENTVALIDATION\": eventvalidation,\n",
      "                          \"__VIEWSTATE\": viewstate\n",
      "                    })\n",
      "\n",
      "    return r.text\n",
      "\n",
      "\n",
      "def test():\n",
      "    data = extract_carriers(html_page)\n",
      "    assert len(data) == 16\n",
      "    assert \"FL\" in data\n",
      "    assert \"NK\" in data\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# All your changes should be in the 'extract_airports' function\n",
      "# It should return a list of airport codes, excluding any combinations like \"All\"\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "html_page = \"options.html\"\n",
      "\n",
      "\n",
      "def extract_airports(page):\n",
      "    data = []\n",
      "    with open(page, \"r\") as html:\n",
      "        # do something here to find the necessary values\n",
      "        soup = BeautifulSoup(html)\n",
      "        \n",
      "        airport_codes = soup.find_all(id='AirportList')[0]\n",
      "        options = airport_codes.find_all('option')\n",
      "        \n",
      "        for option in options:\n",
      "            if 'All' not in option['value']:\n",
      "                data.append(option['value'])\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    data = extract_airports(html_page)\n",
      "    assert len(data) == 15\n",
      "    assert \"ATL\" in data\n",
      "    assert \"ABR\" in data\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# Let's assume that you combined the code from the previous 2 exercises\n",
      "# with code from the lesson on how to build requests, and downloaded all the data locally.\n",
      "# The files are in a directory \"data\", named after the carrier and airport:\n",
      "# \"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
      "# The table with flight info has a table class=\"dataTDRight\".\n",
      "# There are couple of helper functions to deal with the data files.\n",
      "# Please do not change them for grading purposes.\n",
      "# All your changes should be in the 'process_file' function\n",
      "# This is example of the datastructure you should return\n",
      "# Each item in the list should be a dictionary containing all the relevant data\n",
      "# Note - year, month, and the flight data should be integers\n",
      "# You should skip the rows that contain the TOTAL data for a year\n",
      "# data = [{\"courier\": \"FL\",\n",
      "#         \"airport\": \"ATL\",\n",
      "#         \"year\": 2012,\n",
      "#         \"month\": 12,\n",
      "#         \"flights\": {\"domestic\": 100,\n",
      "#                     \"international\": 100}\n",
      "#         },\n",
      "#         {\"courier\": \"...\"}\n",
      "# ]\n",
      "from bs4 import BeautifulSoup\n",
      "from zipfile import ZipFile\n",
      "import os\n",
      "\n",
      "datadir = \"data\"\n",
      "\n",
      "\n",
      "def open_zip(datadir):\n",
      "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def process_all(datadir):\n",
      "    files = os.listdir(datadir)\n",
      "    return files\n",
      "\n",
      "\n",
      "def process_file(f):\n",
      "    # This is example of the datastructure you should return\n",
      "    # Each item in the list should be a dictionary containing all the relevant data\n",
      "    # Note - year, month, and the flight data should be integers\n",
      "    # You should skip the rows that contain the TOTAL data for a year\n",
      "    # data = [{\"courier\": \"FL\",\n",
      "    #         \"airport\": \"ATL\",\n",
      "    #         \"year\": 2012,\n",
      "    #         \"month\": 12,\n",
      "    #         \"flights\": {\"domestic\": 100,\n",
      "    #                     \"international\": 100}\n",
      "    #         },\n",
      "    #         {\"courier\": \"...\"}\n",
      "    # ]\n",
      "    data = []\n",
      "\n",
      "    \n",
      "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
      "\n",
      "        soup = BeautifulSoup(html)\n",
      "        table_rows = soup.find_all('tr', 'dataTDRight')\n",
      "        for row in table_rows:\n",
      "            table_cells = row.children\n",
      "            cell_values = []\n",
      "            for cell in table_cells:\n",
      "                cell_values.append(cell.string)\n",
      "            \n",
      "            if \"TOTAL\" not in cell_values:\n",
      "                info = {}\n",
      "                info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
      "                info['year'] = int(cell_values[1])\n",
      "                info['month'] = int(cell_values[2])\n",
      "                info['flights'] = {}\n",
      "                info['flights']['domestic'] = int(str(cell_values[3]).replace(',',''))\n",
      "                info['flights']['international'] = int(str(cell_values[4]).replace(',',''))\n",
      "                \n",
      "                data.append(info)\n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    print \"Running a simple test...\"\n",
      "    #open_zip(datadir)\n",
      "    files = process_all(datadir)\n",
      "    data = []\n",
      "    for f in files:\n",
      "        data += process_file(f)\n",
      "    assert len(data) == 399\n",
      "    for entry in data[:3]:\n",
      "        assert type(entry[\"year\"]) == int\n",
      "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
      "        assert len(entry[\"airport\"]) == 3\n",
      "        assert len(entry[\"courier\"]) == 2\n",
      "    assert data[-1][\"airport\"] == \"ATL\"\n",
      "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
      "    \n",
      "    print \"... success!\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running a simple test...\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-176-8574a7f4a181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-176-8574a7f4a181>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m399\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# This and the following exercise are using US Patent database.\n",
      "# The patent.data file is a small excerpt of a much larger datafile\n",
      "# that is available for download from US Patent website. They are pretty large ( >100 MB each).\n",
      "# The data itself is in XML, however there is a problem with how it's formatted.\n",
      "# Please run this script and observe the error. Then find the line that is causing the error.\n",
      "# You can do that by just looking at the datafile in the web UI, or programmatically.\n",
      "# For quiz purposes it does not matter, but as an exercise we suggest that you try to do it programmatically.\n",
      "# The original file is ~600MB large, you might not be able to open it in a text editor.\n",
      "\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "PATENTS = 'patent.data'\n",
      "\n",
      "def get_root(fname):\n",
      "\n",
      "    try:\n",
      "        tree = ET.parse(fname)\n",
      "        return tree.getroot()\n",
      "    except ET.ParseError as e:\n",
      "        print e\n",
      "        line_number = [int(s.replace(',','')) for s in str(e).split() if s.replace(',','').isdigit()]\n",
      "\n",
      "        with open(fname) as f:\n",
      "            lines = f.readlines()\n",
      "            print lines[line_number[0] - 1]\n",
      "        \n",
      "\n",
      "\n",
      "get_root(PATENTS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "junk after document element: line 657, column 0\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "                \n",
      "                    \n",
      "                        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
      "# it has several root elements, and XML declarations.\n",
      "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
      "# So, one solution would be to split the file into separate documents,\n",
      "# so that you can process the resulting files as valid XML documents.\n",
      "\n",
      "import xml.etree.ElementTree as ET\n",
      "PATENTS = 'patent.data'\n",
      "\n",
      "def get_root(fname):\n",
      "    tree = ET.parse(fname)\n",
      "    return tree.getroot()\n",
      "\n",
      "\n",
      "def split_file(filename):\n",
      "    # we want you to split the input file into separate files\n",
      "    # each containing a single patent.\n",
      "    # As a hint - each patent declaration starts with the same line that was causing the error\n",
      "    # The new files should be saved with filename in the following format:\n",
      "    # \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
      "\n",
      "    n = 0\n",
      "    with open(filename) as f:\n",
      "        lines = f.readlines()\n",
      "        for line in lines:\n",
      "            if line.strip() == '<?xml version=\"1.0\" encoding=\"UTF-8\"?>':\n",
      "                with open(\"{}-{}\".format(filename,n), 'w') as new_f:\n",
      "                    for line in lines:\n",
      "                        new_f.write(line.strip())\n",
      "                n += 1\n",
      "                        \n",
      "\n",
      "\n",
      "def test():\n",
      "    split_file(PATENTS)\n",
      "    for n in range(4):\n",
      "        try:\n",
      "            fname = \"{}-{}\".format(PATENTS, n)\n",
      "            f = open(fname, \"r\")\n",
      "            if not f.readline().startswith(\"<?xml\"):\n",
      "                print \"You have not split the file {} in the correct boundary!\".format(fname)\n",
      "            f.close()\n",
      "        except:\n",
      "            print \"Could not find file {}. Check if the filename is correct!\".format(fname)\n",
      "\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    }
   ],
   "metadata": {}
  }
 ]
}