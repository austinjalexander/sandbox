{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{w}$: connection weights  \n",
    "$b$: neuron bias  \n",
    "$g(\\cdot)$: activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function examples:  \n",
    "linear: $g(a) = a$  \n",
    "sigmoid: $g(a) = \\text{sigm}(a) = \\frac{1}{1+\\text{exp}(-a)} = \\frac{1}{1+e^{-a}}$  \n",
    "hyperbolic tanget: $g(a) = \\text{tanh}(a) = \\frac{\\text{exp}(a)-\\text{exp}(-a)}{\\text{exp}(a)+\\text{exp}(-a)} = \\frac{\\text{exp}(2a)-1}{\\text{exp}(2a)+1} = \\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}} = \\frac{e^{2a}-1}{e^{2a}+1}$  \n",
    "rectified linear function: $g(a) = \\text{reclin}(a) = \\text{max}(0,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activation function: rectified linear function\n",
    "def g(a):\n",
    "    np.max(0,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron pre-activation  \n",
    "$a(\\textbf{x}) = b + \\sum\\limits_{i} w_{i}x_{i} = b + \\textbf{w}^{T}\\textbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neuron pre-activation (input)\n",
    "def a(x):\n",
    "    return b + w.T.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron activation  \n",
    "$h(\\textbf{x}) = g(a(\\textbf{x}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neuron activation (output)\n",
    "def h(x):\n",
    "    return g(a(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o(\\cdot)$: output activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###single hidden-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden layer pre-activation  \n",
    "$\\textbf{a}(\\textbf{x}) = b^{(1)} + \\textbf{W}^{(1)}\\textbf{x}$  \n",
    "$\\big(\\textbf{a}(\\textbf{x})_{i} = b^{(1)}_{i} + \\sum\\limits_{j} W^{(1)}_{i,j}x_{j}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden layer activation  \n",
    "$\\textbf{h}(\\textbf{x}) = \\textbf{g}(\\textbf{a}(\\textbf{x}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output layer activation  \n",
    "$f(\\textbf{x}) = o\\big(b^{(2)} + \\textbf{w}^{(2)^{T}}\\textbf{h}^{(1)}(\\textbf{x})\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for binary classification, can simply use sigmoid activation function\n",
    "\n",
    "for multi-class classification, need 1 output per class\n",
    "\n",
    "$p(y = c|\\textbf{x}), c \\in {1,...,C}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax activation function: $\\textbf{o}(\\textbf{a}) = \\text{softmax}(\\textbf{a}) = \\big[\\frac{\\text{exp}(a_{1})}{\\sum\\limits_{c}\\text{exp}(a_{c})} ... \\frac{\\text{exp}(a_{C})}{\\sum\\limits_{c}\\text{exp}(a_{c})}\\big]^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###multilayer with $L$ hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer pre-activation for $k>0$, where $\\textbf{h}^{(0)}(\\textbf{x}) = \\textbf{x}$  \n",
    "$\\textbf{a}^{(k)}(\\textbf{x}) = \\textbf{b}^{(k)} + \\textbf{W}^{(k)}\\textbf{h}^{(k-1)}(\\textbf{x})$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden layer activation ($k$ from 1 to $L$)  \n",
    "$\\textbf{h}^{(k)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(k)}(\\textbf{x}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output layer activation ($k=L+1$)  \n",
    "$\\textbf{h}^{(L+1)}(\\textbf{x}) = \\textbf{o}(\\textbf{a}^{(L+1)}(\\textbf{x})) = \\textbf{f}(\\textbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###empirical risk minimization (structural when using regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{\\theta} {\\mathrm{argmin}} \\frac{1}{T}\\sum\\limits_{t}l(f(\\textbf{x}^{(t)}; \\boldsymbol\\theta),y^{(t)}) + \\lambda\\Omega(\\boldsymbol\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol\\theta \\equiv \\{\\textbf{W}^{(1)},\\textbf{b}^{(1)},...,\\textbf{W}^{(L+1)},\\textbf{b}^{(L+1)}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l$: a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Omega(\\boldsymbol\\theta)$: regularizer (penalizes certain values of $\\boldsymbol\\theta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$: hyperparameter that controls the balance between optimizing the average loss and optimizing the regularizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$: hyperparameter learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta} l(f(\\textbf{x}^{(t)}; \\boldsymbol\\theta),y^{(t)})$: computes parameter gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta} \\Omega(\\boldsymbol\\theta)$: gradient of regularizer with respect ot $\\boldsymbol\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-initialize $\\boldsymbol\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-for $N$ iterations  \n",
    "--> for each training example $(x^{(t)},y^{(t)})$  \n",
    "----> $\\Delta = -\\nabla_{\\theta} l(f(\\textbf{x}^{(t)}; \\boldsymbol\\theta),y^{(t)}) - \\lambda \\nabla_{\\theta} \\Omega(\\boldsymbol\\theta)$  \n",
    "----> $\\boldsymbol\\theta \\leftarrow \\boldsymbol\\theta + \\alpha \\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
