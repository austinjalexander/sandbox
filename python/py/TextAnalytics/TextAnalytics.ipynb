{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 files\n",
      "[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "fileids = gutenberg.fileids()\n",
    "print len(fileids), \"files\"\n",
    "print fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_raw = gutenberg.raw(fileids=['carroll-alice.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "print 'type: ', type(alice_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister \n"
     ]
    }
   ],
   "source": [
    "print alice_raw[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 files\n",
      "[u'english-kjv.txt', u'english-web.txt', u'finnish.txt', u'french.txt', u'german.txt', u'lolcat.txt', u'portuguese.txt', u'swedish.txt']\n"
     ]
    }
   ],
   "source": [
    "fileids = genesis.fileids()\n",
    "print len(fileids), \"files\"\n",
    "print fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heaven and the earth.\n",
      "And the earth was without form, and void; and\n",
      "\n",
      "In the beginning God created the heavens and the earth.\n",
      "Now the earth was formless and empty.  Darkn\n",
      "\n",
      "Alussa Jumala loi taivaan ja maan. \n",
      "Maa oli\n",
      "autio ja tyhjä, pimeys peitti syvyydet, ja Jumalan\n",
      "henki\n",
      "\n",
      "Au commencement, Dieu créa les cieux et la terre.\n",
      "La terre était informe et vide: il y avait des tén\n",
      "\n",
      "Am Anfang schuf Gott Himmel und Erde.\n",
      "Und die Erde war wüst und leer, und es war finster auf der Tie\n",
      "\n",
      "Oh hai. In teh beginnin Ceiling Cat maded teh skiez An da Urfs, but he did not eated dem.\n",
      "Da Urfs no\n",
      "\n",
      "No princípio, criou Deus os céus e a terra.\n",
      "E a terra era sem forma e vazia; e {havia} trevas sobre \n",
      "\n",
      "I begynnelsen skapade Gud himmel och jord.\n",
      "Och jorden var öde och tom, och mörker var över djupet, o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fileid in fileids:\n",
    "    print genesis.raw(fileids=[fileid])[:100] + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.bigrams('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('H', 'e')\n",
      "('e', 'l')\n",
      "('l', 'l')\n",
      "('l', 'o')\n"
     ]
    }
   ],
   "source": [
    "for b in text:\n",
    "    print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = nltk.bigrams(['This', 'is', 'gonna', 'be', 'great!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is')\n",
      "('is', 'gonna')\n",
      "('gonna', 'be')\n",
      "('be', 'great!')\n"
     ]
    }
   ],
   "source": [
    "for b in words:\n",
    "    print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "de\n"
     ]
    }
   ],
   "source": [
    "print detect(\"War doesn't show who's right, just who's left.\")\n",
    "print detect(\"Ein, zwei, drei, vier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodecsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "천천히 말씀해 주세요\r\n",
      "Können Sie bitte langsamer sprechen\r\n",
      "麻煩你講慢一點\r\n",
      "تكلم ببطء من فضلك\r\n",
      "Por favor hable más despacio\r\n",
      "ゆっくり話してください\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/7languages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "천천히 말씀해 주세요 | ko\n",
      "Können Sie bitte langsamer sprechen | de\n",
      "麻煩你講慢一點 | ko\n",
      "تكلم ببطء من فضلك | ar\n",
      "Por favor hable más despacio | es\n",
      "ゆっくり話してください | ja\n",
      "\n",
      "['ko', 'de', 'zh', 'ar', 'es', 'ja']\n",
      "['ko', 'de', 'ko', 'ar', 'es', 'ja']\n",
      "\n",
      "   | a d e j k z |\n",
      "   | r e s a o h |\n",
      "---+-------------+\n",
      "ar |<1>. . . . . |\n",
      "de | .<1>. . . . |\n",
      "es | . .<1>. . . |\n",
      "ja | . . .<1>. . |\n",
      "ko | . . . .<1>. |\n",
      "zh | . . . . 1<.>|\n",
      "---+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/7languages.txt', 'rb') as input_file:\n",
    "    row_reader = unicodecsv.reader(input_file)\n",
    "    result = []\n",
    "    for row in row_reader:\n",
    "        lang = detect(row[0])\n",
    "        result += [lang.encode('ascii', 'ignore')]\n",
    "        print row[0], \"|\", lang\n",
    "        \n",
    "    truth = ['ko', 'de', 'zh', 'ar', 'es', 'ja']\n",
    "    print \"\\n\", truth\n",
    "    print result\n",
    "    \n",
    "    print \"\\n\", nltk.ConfusionMatrix(truth, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/7languages.txt', 'rb') as input_file:\n",
    "    row_reader = unicodecsv.reader(input_file)\n",
    "    result = []\n",
    "    for row in row_reader:\n",
    "        lang = detect(row[0])\n",
    "        result += [lang.encode('ascii', 'ignore')]\n",
    "        print row[0], \"|\", lang\n",
    "        \n",
    "    truth = ['ko', 'de', 'zh', 'ar', 'es', 'ja']\n",
    "    print \"\\n\", truth\n",
    "    print result\n",
    "    \n",
    "    print \"\\n\", nltk.ConfusionMatrix(truth, result)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heaven and the earth.\n",
      "And the earth was without form, and void; and | en \n",
      "\n",
      "In the beginning God created the heavens and the earth.\n",
      "Now the earth was formless and empty.  Darkn | en \n",
      "\n",
      "Alussa Jumala loi taivaan ja maan. \n",
      "Maa oli\n",
      "autio ja tyhjä, pimeys peitti syvyydet, ja Jumalan\n",
      "henki | fi \n",
      "\n",
      "Au commencement, Dieu créa les cieux et la terre.\n",
      "La terre était informe et vide: il y avait des tén | fr \n",
      "\n",
      "Am Anfang schuf Gott Himmel und Erde.\n",
      "Und die Erde war wüst und leer, und es war finster auf der Tie | de \n",
      "\n",
      "Oh hai. In teh beginnin Ceiling Cat maded teh skiez An da Urfs, but he did not eated dem.\n",
      "Da Urfs no | en \n",
      "\n",
      "No princípio, criou Deus os céus e a terra.\n",
      "E a terra era sem forma e vazia; e {havia} trevas sobre  | pt \n",
      "\n",
      "I begynnelsen skapade Gud himmel och jord.\n",
      "Och jorden var öde och tom, och mörker var över djupet, o | sv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for fileid in fileids:\n",
    "    lang = detect(genesis.raw(fileids=[fileid])[:100])\n",
    "    result += [lang.encode('ascii', 'ignore')]\n",
    "    print genesis.raw(fileids=[fileid])[:100], \"|\", lang, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'some', '#', '@', '*', '!', '$', 'text', '!', 'This', 'ca', \"n't\", 'be', 'right', '!']\n",
      "['This', 'is', 'some', '#@*!$', 'text', '!', 'This', 'can', \"'\", 't', 'be', 'right', '!']\n"
     ]
    }
   ],
   "source": [
    "some_text = \"This is some #@*!$ text! This can't be right!\"\n",
    "print nltk.word_tokenize(some_text)\n",
    "print nltk.wordpunct_tokenize(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "천천히 |||\n",
      "말씀해 |||\n",
      "주세요 |||\n",
      "\n",
      "Können |||\n",
      "Sie |||\n",
      "bitte |||\n",
      "langsamer |||\n",
      "sprechen |||\n",
      "\n",
      "麻煩你講慢一點 |||\n",
      "\n",
      "تكلم |||\n",
      "ببطء |||\n",
      "من |||\n",
      "فضلك |||\n",
      "\n",
      "Por |||\n",
      "favor |||\n",
      "hable |||\n",
      "más |||\n",
      "despacio |||\n",
      "\n",
      "ゆっくり話してください |||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/7languages.txt', 'rb') as input_file:\n",
    "    row_reader = unicodecsv.reader(input_file)\n",
    "    for row in row_reader:\n",
    "        tokens = nltk.word_tokenize(row[0])\n",
    "        for t in tokens:\n",
    "            print t, \"|||\"\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rosette.api import API, RosetteParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = API(service_url=\"https://api.rosette.com/rest/v1\", user_key=\"40fe14de7872ebf3b8c5e11c17fb7a5f\")\n",
    "params = RosetteParameters()\n",
    "op = api.morphology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "천천히 ||| 말씀해 ||| 주세요 |||\n",
      "Können ||| Sie ||| bitte ||| langsamer ||| sprechen |||\n",
      "麻煩 ||| 你 ||| 講 ||| 慢 ||| 一點 |||\n",
      "تكلم ||| ببطء ||| من ||| فضلك |||\n",
      "Por ||| favor ||| hable ||| más ||| despacio |||\n",
      "ゆっくり ||| 話し ||| て ||| ください |||\n"
     ]
    }
   ],
   "source": [
    "with open('data/7languages.txt', 'rb') as input_file:\n",
    "    row_reader = unicodecsv.reader(input_file)\n",
    "    for row in row_reader:\n",
    "        params[\"content\"] = row[0]\n",
    "        result = op.operate(params)\n",
    "        tokens = result['lemmas']\n",
    "        for t in tokens:\n",
    "            print t['text'], \"|||\",\n",
    "        print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
