{
 "metadata": {
  "name": "",
  "signature": "sha256:66bbb7ccf0ee04cdfe78d5d4c11095b9949610ee518d78140c140b1a460bd231"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Multivariate Linear Regression \n",
      "\n",
      "##predict prices of houses (Ng ML Example 1, part 2)\n",
      "\n",
      "> Suppose you are selling your house, and you want to know what a good market price would \n",
      "> be. One way to do this is to first collect information on recent houses sold and make a model of\n",
      "> housing prices.\n",
      "\n",
      "> The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first \n",
      "> column is the size of the house (in square feet), the second column is the number of bedrooms,\n",
      "> and the third column is the price of the house."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "# import data\n",
      "data = pd.read_csv('mlclass-ex1/ex1data2.txt', header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Notation\n",
      "Features: $x_{1}, x_{2}, x_{3}, ..., x_{n}$  \n",
      "Prediction: $y$  \n",
      " \n",
      "$m =$ number of training examples  \n",
      "$n =$ number of features  \n",
      "$x^{(i)} =$ input (features) of the $i$-th training example (the superscript is the index into the training set)  \n",
      "$x_{j}^{(i)} =$ value of feature $j$ in the $i$-th training example\n",
      "\n",
      "For this data set:  \n",
      "$x_{1}$: size (feet$^{2}$)  \n",
      "$x_{2}$: # of bedrooms   \n",
      "$y$: price ($)\n",
      "\n",
      "####Feature Scaling Mean Normalization\n",
      "$x_{j}^{(i)} = \\frac{x_{j}^{(i)} - \\bar{x}_{j}}{s_{j}}$\n",
      "\n",
      "###Hypothesis\n",
      "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$ \n",
      "\n",
      "For convienience of notation, define $x_{0} = 1$, so that $x_{0}^{(i)} = 1$  \n",
      "\n",
      "Now, the feature vector can be zero-indexed:\n",
      "\n",
      "$x^{(i)} = \n",
      "\\begin{bmatrix}\n",
      "x_{0} \\\\\n",
      "x_{1} \\\\\n",
      "x_{2} \\\\\n",
      "... \\\\\n",
      "x_{n}\n",
      "\\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
      "\n",
      "And, since the parameter vector is zero-indexed:\n",
      "\n",
      "$\\theta = \n",
      "\\begin{bmatrix}\n",
      "\\theta_{0} \\\\\n",
      "\\theta_{1} \\\\\n",
      "\\theta_{2} \\\\\n",
      "... \\\\\n",
      "\\theta_{n}\n",
      "\\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
      "\n",
      "The new hypothesis form is: $h_{\\theta}(x) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$\n",
      "\n",
      "####Hypothesis Calculation\n",
      "\n",
      "Construct the following ($m\\times[n+1]$) design matrix:\n",
      "\n",
      "$X = \n",
      "\\begin{bmatrix}\n",
      "x_{0}^{(0)} && x_{1}^{(0)} && ... && x_{n}^{(0)} \\\\\n",
      "x_{0}^{(1)} && x_{1}^{(1)} && ... && x_{n}^{(1)} \\\\\n",
      "x_{0}^{(2)} && x_{1}^{(2)} && ... && x_{n}^{(2)} \\\\\n",
      "... && ... && ... && ... \\\\\n",
      "x_{0}^{(m-1)} && x_{1}^{(m-1)} && ... && x_{n}^{(m-1)}\n",
      "\\end{bmatrix}$\n",
      "\n",
      "To create a ($m\\times1$) hypothesis matrix, perform matrix-vector multiplication:\n",
      "\n",
      "$X \\times \\theta = \n",
      "\\begin{bmatrix}\n",
      "x_{0}^{(0)} \\times \\theta_{0} + x_{1}^{(0)} \\times \\theta_{1} \\\\\n",
      "x_{0}^{(1)} \\times \\theta_{0} + x_{1}^{(1)} \\times \\theta_{1} \\\\\n",
      "x_{0}^{(2)} \\times \\theta_{0} + x_{1}^{(2)} \\times \\theta_{1} \\\\\n",
      "... \\\\\n",
      "x_{0}^{(m-1)} \\times \\theta_{0} + x_{1}^{(m-1)} \\times \\theta_{1}\n",
      "\\end{bmatrix}$\n",
      "\n",
      "Alternatively, to compute each hypothesis return value per training sample: \n",
      "\n",
      "Perform the inner/dot/scalar product of vectors of $\\theta$-transpose and $X^{(i)}$:\n",
      "\n",
      "$\\theta^{T} = \\begin{bmatrix}\\theta_{0}&&\\theta_{1}&&\\theta_{2}&&...&&\\theta_{n}\\end{bmatrix}$\n",
      "\n",
      "$h_{\\theta}(x) = \\theta^{T}X^{(i)} = \\begin{bmatrix}\\theta_{0}&&\\theta_{1}&&\\theta_{2}&&...&&\\theta_{n}\\end{bmatrix} \\times \\begin{bmatrix}\n",
      "x_{0}^{(i)} \\\\\n",
      "x_{1}^{(i)} \\\\\n",
      "x_{2}^{(i)} \\\\\n",
      "... \\\\\n",
      "x_{n}^{(i)}\n",
      "\\end{bmatrix}$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initial data vectors\n",
      "x_1 = data[0].values\n",
      "x_2 = data[1].values\n",
      "y = data[2].values\n",
      "\n",
      "# training sample size\n",
      "m = data[0].count()\n",
      "\n",
      "# number of features\n",
      "n = 2\n",
      "\n",
      "# proper vector shapes\n",
      "x_1.shape = (m,1)\n",
      "x_2.shape = (m, 1)\n",
      "y.shape = (m,1)\n",
      "\n",
      "# Feature Scaling (unnecessary if using the normal equation method)\n",
      "# option1: normalization\n",
      "#x_1 = np.true_divide(x_1, np.amax(x_1))\n",
      "#x_2 = np.true_divide(x_2, np.amax(x_2))\n",
      "# option2: mean normalization\n",
      "xbar_1 = np.mean(x_1)\n",
      "xbar_2 = np.mean(x_2)\n",
      "\n",
      "s_1 = np.std(x_1)\n",
      "s_2 = np.std(x_2)\n",
      "\n",
      "x_1 = np.divide((np.subtract(x_1,xbar_1)),s_1)\n",
      "x_2 = np.divide((np.subtract(x_2,xbar_2)),s_2)\n",
      "\n",
      "# design matrix\n",
      "X = np.hstack((np.ones((m,1)),x_1,x_2))\n",
      "\n",
      "# theta parameters \n",
      "theta = np.zeros(((n+1),1))\n",
      "\n",
      "#gradient descent, number of iterations\n",
      "iterations = 50\n",
      "\n",
      "# learning rates \n",
      "alpha = [-0.3, -0.1, -0.03, -0.01, -0.003, -0.001, 0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n",
      "alpha = [0.3]\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Cost Function (iterative and vectorized forms)\n",
      "$J(\\theta_{0},\\theta_{1},...,\\theta_{n}) = J(\\theta) = \\frac{1}{2m}\\sum\\limits_{i=0}^{m-1}(h_{\\theta}(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2m}\\sum\\limits_{i=0}^{m-1}(\\theta^{T}X^{(i)} - y^{(i)})^2 = \\frac{1}{2m}(X\\theta - y)^{T}(X\\theta - y)$\n",
      "\n",
      "###Vectorized Implementation of Gradient Descent\n",
      "####Minimize the objective/cost function $J$\n",
      "$\\delta = \n",
      "\\begin{bmatrix}\n",
      "\\delta_{0} \\\\\n",
      "\\delta_{1} \\\\\n",
      "\\delta_{2} \\\\\n",
      "... \\\\\n",
      "\\delta_{n}\n",
      "\\end{bmatrix}$\n",
      "\n",
      "$\\delta_{j} = \\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(\\theta^{T}X^{(i)} - y^{(i)}) \\cdot X_{j}^{(i)}$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cost function: iterative implementation\n",
      "    #def J():\n",
      "    #    summation = 0\n",
      "    #    for i in xrange(0,m):\n",
      "    #        summation += ((theta.T.dot(X[i])) - y[i])**2\n",
      "    #        \n",
      "    #    return (1.0/(2*m)) * summation\n",
      "    # \n",
      "    \n",
      "# cost function: vectorized implementation\n",
      "def J():\n",
      "    return (1.0/(2*m)) * (((X.dot(theta)) - y).T).dot(X.dot(theta) - y)\n",
      "\n",
      "# delta-vector function for derivatives\n",
      "def deltas():\n",
      "    delta = np.zeros(((n+1),1))\n",
      "    for j in xrange(0,(n+1)):\n",
      "        summation = 0\n",
      "        #for i in xrange(0,m):\n",
      "        #summation += ((theta.T.dot(X[i]) - y[i]) * X[i][j])\n",
      "        \n",
      "        summation = np.sum((X.dot(theta) - y) * X[:,j])\n",
      "        \n",
      "        delta[j] = (1.0/m) * summation\n",
      "    print np.sum(delta)\n",
      "    return delta\n",
      "\n",
      "# vectorized delta function\n",
      "def delta():\n",
      "    return (1.0/m) * (X.T.dot((X.dot(theta)) - y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial\\theta_{j}} J(\\theta) = \\theta_{j} - \\alpha \\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_{j}^{(i)} = \\theta_{j} - \\alpha \\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(\\theta^{T}x^{(i)} - y^{(i)}) \\cdot x_{j}^{(i)} = \\theta_{j} - \\alpha\\delta_{j}$ \n",
      "\n",
      "repeat until convergence {  \n",
      "$\\theta := \\theta - \\alpha \\delta$  \n",
      "}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# gradient descent, test multiple alphas\n",
      "for a in xrange(0,len(alpha)):\n",
      "    \n",
      "    # reset theta\n",
      "    theta = np.zeros(((n+1),1))\n",
      "    \n",
      "    # reset vector J_values, store cost function values for plotting\n",
      "    J_values = np.zeros((iterations,1))\n",
      "    \n",
      "    for iteration in xrange(0,iterations):\n",
      "        theta = theta - (alpha[a] * delta())\n",
      "        J_values[iteration] = J()\n",
      "    \n",
      "    # visualize the cost function (2-D)\n",
      "    cost_x = np.arange(iterations)\n",
      "    cost_x.shape = (iterations,1)\n",
      "    plt.plot(cost_x,J_values)\n",
      "    plt.title(\"Learning Rate: \" + str(alpha[a]))\n",
      "    plt.xlabel('iterations')\n",
      "    plt.ylabel(r\"$J(\\theta)$\")\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEZCAYAAACJjGL9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1088f3810>"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict price of house with 1650 feet^2 and 3 bedrooms\n",
      "x = np.array([[1.0],\n",
      "              [1650],\n",
      "              [3]])\n",
      "\n",
      "# mean normalize (unnecessary if using the normal equation method)\n",
      "x[1] = np.true_divide((np.subtract(x[1],xbar_1)),s_1)\n",
      "x[2] = np.true_divide((np.subtract(x[2],xbar_2)),s_2)\n",
      "\n",
      "# compute and display prediction\n",
      "y_hat = theta.T.dot(x)\n",
      "print \"\\nPrediction for 1650 feet^2 and 3 bedrooms: \" + str(y_hat) + \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Prediction for 1650 feet^2 and 3 bedrooms: [[ 293092.21273076]]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Normal Equation (an alternative to gradient descent)\n",
      "\n",
      "Return the value of $\\theta$ that minimizes the cost function:  \n",
      "$\\theta = (X^{T}X)^{-1}X^{T}y$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "normal_eq_theta = (np.linalg.pinv(X.T.dot(X)).dot(X.T)).dot(y)\n",
      "\n",
      "normal_eq_y_hat = normal_eq_theta.T.dot(x)\n",
      "print \"\\n\\nPrediction for 1650 feet^2 and 3 bedrooms: \" + str(normal_eq_y_hat) + \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Prediction for 1650 feet^2 and 3 bedrooms: [[ 293081.4643349]]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "last equality NOT FROM Ng (from Wikipedia)\n",
      "\n",
      "$\\theta = (X^{T}X)^{-1}X^{T}y = X^{+}y$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "normal_eq_theta = (np.linalg.pinv(X).dot(y))\n",
      "\n",
      "normal_eq_y_hat = normal_eq_theta.T.dot(x)\n",
      "print \"\\n\\nPrediction for 1650 feet^2 and 3 bedrooms: \" + str(normal_eq_y_hat) + \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Prediction for 1650 feet^2 and 3 bedrooms: [[ 293081.4643349]]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 66
    }
   ],
   "metadata": {}
  }
 ]
}